{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac347d56",
   "metadata": {},
   "source": [
    "## Clone the repository by running the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf852ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup: clone lab repo and enter it ---\n",
    "!git clone https://github.com/dontolon/ml_beam_management_lab.git\n",
    "%cd ml_beam_management_lab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848f45da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/dante/experiments/lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea82e85",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c046d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy>=1.22 pandas>=1.5 matplotlib>=3.6 torch>=2.0 scikit-learn>=1.2 utm>=0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14f21bb",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be54311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from helper_scripts.load_data import normalize_pos, min_max, build_mlp\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ea3a0c",
   "metadata": {},
   "source": [
    "#### Load the data\n",
    "\n",
    "We will import and load scenario 2 from the DeepSense6G dataset (https://www.deepsense6g.net/scenario-2/) \n",
    "\n",
    "This scenario emulates a Vehicle-to-Infrastructure (V2I) mmWave communication setup. \n",
    "\n",
    "We consider a system where a base station (BS) with $N$ antennas communicates with a single-antenna UE using one of the $M$ beamforming vectors $\\mathbf{f}_m \\in \\mathbb{C}^{N \\times 1}$ present in its codebook \n",
    "$\\mathcal{F} = \\{\\mathbf{f}_m\\}_{m=1}^M$.\n",
    "\n",
    "In our case:  \n",
    "- $M = 64$ beamforming vectors,  \n",
    "- $N = 16$ antennas at the BS,  \n",
    "- carrier frequency is **60 GHz**.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b5e5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "# Base station coordinates\n",
    "BS_location = np.array([[33.42034722222222, -111.92915277777779]])\n",
    "\n",
    "# UE Positions (lat, lon)\n",
    "UE_locations = np.load(DATA_DIR / \"scenario2_unit2_loc_1-2974.npy\")[:, :2]\n",
    "\n",
    "# Beam powers (linear scale)\n",
    "beam_powers = np.load(DATA_DIR / \"scenario2_unit1_pwr_60ghz_1-2974.npy\")\n",
    "\n",
    "print(\"BS_location:\", BS_location.shape)\n",
    "print(\"UE_locations:\", UE_locations.shape)\n",
    "print(\"beam_powers:\", beam_powers.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4135672",
   "metadata": {},
   "source": [
    "#### Preparing the data: BS and UE positions\n",
    "\n",
    "In this step, we prepare the spatial data before visualization and modeling:\n",
    "\n",
    "- We **stack** the base station (BS) and user equipment (UE) positions together into one array.  \n",
    "- We then apply **min–max normalization** across all coordinates so that the BS and UE positions are represented in a comparable, normalized coordinate system between 0 and 1.  \n",
    "\n",
    "This ensures that:\n",
    "- Both BS and UE positions are expressed on the same scale.  \n",
    "- Visualization and downstream models (e.g., beam selection models) are not biased by raw latitude/longitude magnitudes.  \n",
    "\n",
    "After normalization:\n",
    "- The **first row** corresponds to the BS.  \n",
    "- The **remaining rows** correspond to the UEs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dd56f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack BS and UEs together\n",
    "all_positions = np.vstack([BS_location, UE_locations])  # (1+N, 2)\n",
    "\n",
    "# Min-max normalize everything\n",
    "all_norm = min_max(all_positions, axis=0)\n",
    "\n",
    "# First row is BS, rest are UEs\n",
    "BS_norm = all_norm[0:1]\n",
    "UE_norm = all_norm[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6ff0e3",
   "metadata": {},
   "source": [
    "#### Visualizing the normalized positions\n",
    "\n",
    "Here, we create a scatter plot to visualize the **normalized positions** of the BS and UEs:\n",
    "\n",
    "- The BS is shown as a red star.  \n",
    "- The UEs are shown as small blue dots.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a550d0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(UE_norm[:,0], UE_norm[:,1], c=\"blue\", s=8, label=\"UEs\")\n",
    "plt.scatter(BS_norm[0,0], BS_norm[0,1], c=\"red\", marker=\"*\", s=200, label=\"BS\")\n",
    "plt.xlabel(\"Normalized Lat\")\n",
    "plt.ylabel(\"Normalized Long\")\n",
    "plt.title(\"UE positions + BS (min–max normalized together)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2a9121",
   "metadata": {},
   "source": [
    "#### Exploring beam power profiles \n",
    "\n",
    "Let's randomly pick one sample from the dataset and explore:\n",
    "\n",
    "1. **Beam power profile (left plot):**  \n",
    "   - Shows the received power values across all 64 beams for the chosen UE.  \n",
    "   \n",
    "2. **Spatial view (right plot):**  \n",
    "   - Shows all normalized UE positions (blue dots).  \n",
    "   - The BS is marked with a red star.  \n",
    "   - The randomly selected UE is highlighted in orange. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecd82fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, beam_powers.shape[0])\n",
    "powers = beam_powers[idx]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14,5))\n",
    "best_idx = np.argmax(powers)\n",
    "\n",
    "# --- Left: beam power profile ---\n",
    "axes[0].plot(range(1, len(powers)+1), powers, marker=\"o\")\n",
    "axes[0].axvline(best_idx+1, color=\"red\", linestyle=\"--\", label=f\"Best beam {best_idx+1}\")\n",
    "axes[0].set_xlabel(\"Beam index (1–64)\")\n",
    "axes[0].set_ylabel(\"Power value\")\n",
    "axes[0].set_title(f\"UE {idx}: Power across beams\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# --- Right: spatial scatter ---\n",
    "axes[1].scatter(UE_norm[:,0], UE_norm[:,1], c=\"blue\", s=8, label=\"UEs\")\n",
    "axes[1].scatter(BS_norm[0,0], BS_norm[0,1], c=\"red\", marker=\"*\", s=200, label=\"BS\")\n",
    "axes[1].scatter(UE_norm[idx,0], UE_norm[idx,1], c=\"orange\", edgecolor=\"black\", s=80, label=\"Selected UE\")\n",
    "axes[1].set_xlabel(\"Normalized x\")\n",
    "axes[1].set_ylabel(\"Normalized y\")\n",
    "axes[1].set_title(\"UE positions + BS (selected UE highlighted)\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1602fe3f",
   "metadata": {},
   "source": [
    "#### Constructing the beam prediction dataset\n",
    "\n",
    "So far, we’ve visualized single UEs and their beam power profiles.  \n",
    "Now we take the next step: create the dataset for supervised learning.\n",
    "\n",
    "- For each sample, we find its **best beam index**:  \n",
    "$$\n",
    "m^* = \\arg\\max_{m \\in \\{1, \\dots, M\\}} \\; \\text{beam\\_powers}_{m},\n",
    "$$\n",
    "\n",
    "- We then define:  \n",
    "  - **Features (X):** the normalized UE positions (shape = (N, 2))  \n",
    "  - **Labels (y):** the best beam index for each sample (shape = (N,))  \n",
    "\n",
    "This dataset \\((X, y)\\) is the starting point for training models to **predict the best beam** from position information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fe5cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_beam = np.argmax(beam_powers, axis=1)\n",
    "X = UE_norm      # shape (N,2)\n",
    "y = best_beam    # shape (N,)\n",
    "\n",
    "print(\"Dataset created:\")\n",
    "print(\"X:\", X.shape, \"y:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a751406",
   "metadata": {},
   "source": [
    "#### Visualizing spatial patterns of best beams\n",
    "\n",
    "Now that we have constructed the dataset \\((X, y)\\), we can visualize how the **best beam index** varies across space:\n",
    "\n",
    "- Each sample is plotted at its normalized position \\((x, y)\\).  \n",
    "- The color encodes the **best beam index** \\(m^*\\).  \n",
    "- The colorbar on the right shows the mapping from color → beam index.  \n",
    "\n",
    "This plot lets us see the **spatial partitioning of beams**:  \n",
    "- Neighboring samples often share the same best beam.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7040c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sc = plt.scatter(X[:,0], X[:,1], c=y, cmap=\"viridis\", s=8)\n",
    "plt.colorbar(sc, label=\"Best beam index\")\n",
    "plt.xlabel(\"Normalized x\")\n",
    "plt.ylabel(\"Normalized y\")\n",
    "plt.title(\"UE positions colored by best beam\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a456acf",
   "metadata": {},
   "source": [
    "#### Preparing the dataset for PyTorch\n",
    "\n",
    "Now that we have the feature matrix $X$ (UE positions) and the labels $y$ (best beam indices), we need to prepare them for training a neural network in PyTorch.\n",
    "\n",
    "Steps:\n",
    "1. **Convert to tensors:**  \n",
    "   - `X_tensor`: the input features (shape: $N \\times 2$).  \n",
    "   - `y_tensor`: the target labels (shape: $N$, with values in $\\{0, \\dots, M-1\\}$).\n",
    "\n",
    "2. **Bundle into a dataset:**  \n",
    "   - `TensorDataset` combines the inputs and labels into a format that PyTorch understands.\n",
    "\n",
    "3. **Wrap with a DataLoader:**  \n",
    "   - Handles batching (here: 128 samples per batch).  \n",
    "   - Shuffles the data each epoch for better generalization.  \n",
    "   - Makes it easy to iterate over the dataset in training loops.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4833e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Bundle into a dataset\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# Create a DataLoader (mini-batches)\n",
    "loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "print(\"Dataset ready for NN training!\")\n",
    "print(\"Number of batches:\", len(loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00122c52",
   "metadata": {},
   "source": [
    "### Splitting the dataset into train/validation/test\n",
    "\n",
    "To properly train and evaluate our model, we need to split the dataset into three parts:\n",
    "\n",
    "1. **Training set (70%)**  \n",
    "   - Used by the neural network to learn the mapping from UE positions → best beam indices.\n",
    "\n",
    "2. **Validation set (15%)**  \n",
    "   - Used to tune hyperparameters and prevent overfitting.  \n",
    "   - The model never directly trains on this data.\n",
    "\n",
    "3. **Test set (15%)**  \n",
    "   - Held out until the very end.  \n",
    "   - Used to evaluate the *true generalization performance*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0149374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70/30 split first\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# Then split 30 into 15/15\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"Train:\", X_train.shape, y_train.shape)\n",
    "print(\"Val:  \", X_val.shape, y_val.shape)\n",
    "print(\"Test: \", X_test.shape, y_test.shape)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "def make_loader(X, y, batch_size=32, shuffle=False):\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "    return DataLoader(TensorDataset(X_tensor, y_tensor), batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "train_loader = make_loader(X_train, y_train, batch_size=32, shuffle=True)\n",
    "val_loader   = make_loader(X_val,   y_val,   batch_size=32)\n",
    "test_loader  = make_loader(X_test,  y_test,  batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a13681",
   "metadata": {},
   "source": [
    "#### Building the model and training components\n",
    "\n",
    "The helper function `build_mlp(...)` constructs not only the neural network architecture, but also the essential components for training:\n",
    "\n",
    "- **Model:** a configurable MLP with user-defined depth, width, and activation.  \n",
    "- **Loss function (criterion):** categorical cross-entropy, suitable for multi-class classification.  \n",
    "- **Optimizer:** Adam, with configurable learning rate.  \n",
    "- **Scheduler:** a multi-step learning rate scheduler that reduces the learning rate by a factor $\\gamma$ at predefined milestones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39679cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, criterion, optimizer, scheduler = build_mlp(\n",
    "    input_dim=2,\n",
    "    output_dim=64,\n",
    "    hidden_dim=256,\n",
    "    hidden_layers=3,\n",
    "    activation=\"ReLU\",\n",
    "    lr=1e-2,\n",
    "    milestones=[20, 40],\n",
    "    gamma=0.2,\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a0458a",
   "metadata": {},
   "source": [
    "#### Training loop and evaluation protocol\n",
    "\n",
    "We now implement the complete training and evaluation routine for the beam prediction model.\n",
    "\n",
    "**1. Training step (`run_epoch`)**\n",
    "- Iterates over one pass of the dataset (an epoch).  \n",
    "- Sets the model to training or evaluation mode.  \n",
    "- For each mini-batch:\n",
    "  - Computes forward pass (`out = model(xb)`).  \n",
    "  - Evaluates the loss using categorical cross-entropy.  \n",
    "  - If training:\n",
    "    - Clears gradients (`optimizer.zero_grad()`).  \n",
    "    - Backpropagates (`loss.backward()`).  \n",
    "    - Updates parameters (`optimizer.step()`).  \n",
    "- Tracks average loss and top-1 accuracy for the epoch.\n",
    "\n",
    "**2. Evaluation step (`evaluate`)**\n",
    "- Runs the model in inference mode (no gradient computation).  \n",
    "- Computes **top-k accuracies** for several values of \\(k\\) (1, 3, 5, 10):  \n",
    "  - For each sample, checks whether the ground-truth beam index appears among the model’s top-\\(k\\) predicted beams.  \n",
    "- Returns a dictionary mapping \\(k \\mapsto \\text{accuracy}\\).\n",
    "\n",
    "**3. Training loop**\n",
    "- Repeats for a fixed number of epochs (here, 60).  \n",
    "- Each epoch:\n",
    "  - Calls `run_epoch` on the training set.  \n",
    "  - Evaluates on the validation set using `evaluate`.  \n",
    "  - Updates the learning rate via the scheduler.  \n",
    "- Records the training history (loss, accuracy) and validation metrics.  \n",
    "- Periodically prints progress (every 10 epochs, and during the first 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014b40c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(loader, train=True):\n",
    "    losses, correct, total = [], 0, 0\n",
    "    if train: model.train()\n",
    "    else: model.eval()\n",
    "\n",
    "    for xb, yb in loader:\n",
    "        if train: optimizer.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = criterion(out, yb)\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == yb).sum().item()\n",
    "        total += len(yb)\n",
    "    return np.mean(losses), correct/total\n",
    "\n",
    "EPOCHS = 60\n",
    "train_hist, val_hist = [], []\n",
    "\n",
    "def evaluate(loader, model, k_values=[1,3,5,10]):\n",
    "    \"\"\"\n",
    "    Evaluate top-k accuracies.\n",
    "    Returns dict {k: acc}.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct_topk = {k: 0 for k in k_values}\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            out = model(xb)                       # logits\n",
    "            _, pred_topk = out.topk(max(k_values), dim=1)  # top-k predictions\n",
    "            total += yb.size(0)\n",
    "            for k in k_values:\n",
    "                correct_topk[k] += (pred_topk[:, :k] == yb.unsqueeze(1)).any(dim=1).sum().item()\n",
    "\n",
    "    return {k: correct_topk[k]/total for k in k_values}\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    tr_loss, tr_acc = run_epoch(train_loader, train=True)\n",
    "    val_metrics = evaluate(val_loader, model, k_values=[1,3,5,10])\n",
    "    scheduler.step()\n",
    "\n",
    "    train_hist.append((tr_loss, tr_acc))\n",
    "    val_hist.append(val_metrics)\n",
    "\n",
    "    if (epoch+1) % 10 == 0 or epoch < 5:\n",
    "        print(f\"Epoch {epoch+1:02d}: Train acc {tr_acc:.3f}, \"\n",
    "              f\"Val top-1 {val_metrics[1]:.3f}, top-3 {val_metrics[3]:.3f}, \"\n",
    "              f\"top-5 {val_metrics[5]:.3f}, top-10 {val_metrics[10]:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e772976",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot([v[1] for v in val_hist], label=\"Top-1\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation accuracy\")\n",
    "plt.title(\"Validation Top-k Accuracies\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
